\thispagestyle{plain}

\subsection{Regression methods}
\noindent While numerous regression methods exist, in this particular context, 
we will narrow our focus to three fundamental techniques, Ordinary Least Squares, Ridge and Lasso Regression."
\subsubsection{Ordinary least squares (OLS)}
\noindent Ordinary Least Squares (OLS) stands as a fundamental technique in regression
analysis, where the aim is to create a model that minimize the diffrense from
the observed data and the predicted model. 

%OLS is a linear model which means that it assumes that the regression function 
%is linear in the inputs $X_1, X_2 \dots X_n$

A linear regression model has the following form :
\begin{align}
    f(\textbf{X}) = \beta_0 + \sum^{p}_{i=1} X_i \beta_i \cite{hastie_09_elements-of.statistical-learning}
\end{align}
Where $\beta$ is the coefficients and X the design matrix. It is importent to note 
that thise method assume that either the function is linear or approxematly linear.

 
The cost function for OLS is given by:
\begin{equation}
    C(\beta) = \frac{1}{n} = \left\lbrace ( \textbf{y} - \textbf{X}\beta  )^T (\textbf{y} - \textbf{X}\beta)\right\rbrace
\end{equation}
From this we obtein that the expression for the optimal $\beta$ is given by:
\begin{equation}
    \beta = (\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T \textbf{y}
\end{equation}
\subsubsection{Ridge}
\begin{equation}
    \beta = (\textbf{X}^T \textbf{X} + \lambda \textbf{I})^{-1}\textbf{X}^T \textbf{y}
\end{equation}

Calculations for bothe the optimal $\beta$ and variance can be found in the apendix
\subsubsection{LASSO}

\subsection{MSE}


\subsection{Resampling techniques}

\noindent The main restriction in machine learning is 
the amount of data points available to create the model out of. It may be the case where one have
done a costfull and time consuming experiments and are left with a small number of data. 
It is therfull extremely useful to have methods where one can reuse the data multiple times
thereby creating a relatively large dataset from the small number of datapoints. In this report
we are going to use two different methods, the first is called bootstrap and the second one is cross validation.

\subsubsection{Bootstrap} 
\noindent The bootstrap method is a resampling procedure that uses data from one 
sample to generate a sampling distribution by repeatedly taking random 
samples from the known sample, with replacement\cite{PSU} This means that if we have a data set D with n data points.
The elements in this data set can be representatet in the following way:
\begin{align}
    D = {d_1, d_2, d_3, d_4, \dots, d_n}
\end{align}
Then by apllying the bootstarp method on this data set one possible output $D^{*}$ can be:
\begin{align}
    D^{*} = {d_3, d_n, d_4, d_4 , \dots, d_2}
\end{align}
From this example we see that one observation can appear multiple times in the new dataset.
We can take this method a step further and create "new" data points by extracting multible data points from 
the dataset and take the mean of all thise values:
\begin{align}
    d_{new} = \frac{1}{k}(d_1, \dots d_k)
\end{align}
This gives us a method of producing lots of "new" data-set from 
limited data points to train our model with. For each of this data-sets
the mean and standard deviation can be calculated to evaluate the model statistically. \cite{MLM}


One huge advatage of using the boostrap method is that the data can be split in to 
test and train before shuffeling the data, this means that the test data can bee kept 
entirely separate from the creation of the model. When we den test the model it will
be on a dataset that has nothing to do with crating the model and will therfore show
how god the model represent real data. 
%\newline
\textbf{Write something about large numbers law, also disadvantages and advantage}
%\newline


\subsubsection{Cross validation}
\noindent Cross validation is another method of creating "new" datasets from the original data. This method 
wrks by splitting the data in k-folds



\subsection{Bias-variance trade-off}
The Bias-varinace trade-off is a measuerment on how accuratly a model fit the real data.

