\thispagestyle{plain}

\subsection{Linear regression methods}

\noindent Linear regression is a foundational statistical modeling technique employed for the prediction of continuous target variables
based on one or more input features. It operates under the assumption that there exists a linear relationship between the input features
and the target variable, which is mathematically expressed as:

\[
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p
\]

In this equation, $\hat{y}$ denotes the predicted target variable, while $x_1, x_2, \ldots, x_p$ represent the input features. The 
coefficients $\beta_0, \beta_1, \beta_2, \ldots, \beta_p$ are parameters that correspond to each respective feature. The primary objective 
of linear regression is to determine these coefficients to establish the optimal linear model that best fits the given data.

Linear regression encompasses several variants, each offering unique characteristics and advantages. In this particular context, 
we will narrow our focus to three fundamental techniques, Ordinary Least Squares (OLS), Ridge regression, and Lasso regression. These 
techniques will be employed in the context of analyzing the two-dimensional Franke function. 


\subsubsection{Ordinary least squares (OLS)}
\noindent Ordinary Least Squares (OLS) stands as a fundamental technique in regression
analysis, where the aim is to create a model that minimize the diffrense from
the observed data and the predicted model. 

%OLS is a linear model which means that it assumes that the regression function 
%is linear in the inputs $X_1, X_2 \dots X_n$

A linear regression model has the following form :
\begin{align}
    f(\textbf{X}) = \beta_0 + \sum^{p}_{i=1} X_i \beta_i \cite{hastie_09_elements-of.statistical-learning}
\end{align}
Where $\beta$ is the coefficients and X the design matrix. It is importent to note 
that thise method assume that either the function is linear or approxematly linear.

 
The cost function for OLS is given by:
\begin{equation}
    C(\beta) = \frac{1}{n} = \left\lbrace ( \textbf{y} - \textbf{X}\beta  )^T (\textbf{y} - \textbf{X}\beta)\right\rbrace
\end{equation}
From this we obtein that the expression for the optimal $\beta$ is given by:
\begin{equation}
    \beta = (\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T \textbf{y}
\end{equation}
\subsubsection{Ridge}
\begin{equation}
    \beta = (\textbf{X}^T \textbf{X} + \lambda \textbf{I})^{-1}\textbf{X}^T \textbf{y}
\end{equation}

Calculations for bothe the optimal $\beta$ and variance can be found in the apendix
\subsubsection{LASSO}

\subsection{MSE}


\subsection{Resampling techniques}

\noindent The main restriction in machine learning is 
the amount of data points available to create the model out of. It may be the case where one have
done a costfull and time consuming experiments and are left with a small number of data. 
It is therfull extremely useful to have methods where one can reuse the data multiple times
thereby creating a relatively large dataset from the small number of datapoints. In this report
we are going to use two different methods, the first is called bootstrap and the second one is cross validation.

\subsubsection{Bootstrap} 
\noindent The bootstrap method is a resampling procedure that uses data from one 
sample to generate a sampling distribution by repeatedly taking random 
samples from the known sample, with replacement\cite{PSU} This means that if we have a data set D with n data points.
The elements in this data set can be representatet in the following way:
\begin{align}
    D = {d_1, d_2, d_3, d_4, \dots, d_n}
\end{align}
Then by apllying the bootstarp method on this data set one possible output $D^{*}$ can be:
\begin{align}
    D^{*} = {d_3, d_n, d_4, d_4 , \dots, d_2}
\end{align}
From this example we see that one observation can appear multiple times in the new dataset.
We can take this method a step further and create "new" data points by extracting multible data points from 
the dataset and take the mean of all thise values:
\begin{align}
    d_{new} = \frac{1}{k}(d_1, \dots d_k)
\end{align}
This gives us a method of producing lots of "new" data-set from 
limited data points to train our model with. For each of this data-sets
the mean and standard deviation can be calculated to evaluate the model statistically. \cite{MLM}


One huge advatage of using the boostrap method is that the data can be split in to 
test and train before shuffeling the data, this means that the test data can bee kept 
entirely separate from the creation of the model. When we den test the model it will
be on a dataset that has nothing to do with crating the model and will therfore show
how god the model represent real data. 
%\newline
\textbf{Write something about large numbers law, also disadvantages and advantage}
%\newline


\subsubsection{Cross validation}
\noindent Cross validation (CV), specifically k-fold cross validation, is a technique designed to assess how well a model will generalize to an independent dataset. Unlike the bootstrap method, which samples with replacement, CV partitions the original dataset into \(k\) distinct subsets or "folds". One of these folds is kept as the test set, while the remaining \(k-1\) folds are used for training the model. This process is repeated \(k\) times, each time with a different fold reserved as the test set and the remaining folds as the training set. Given a dataset \(D\) with \(n\) data points represented as:
\begin{align}
    D = \{d_1, d_2, d_3, \dots, d_n\}
\end{align}
For \(k=5\), an example partition might be:
\begin{align}
    \text{Fold 1} &= \{d_1, d_2, \dots, d_{\frac{n}{5}}\} \\
    \text{Fold 2} &= \{d_{\frac{n}{5}+1}, d_{\frac{n}{5}+2}, \dots, d_{2 \times \frac{n}{5}}\} \\
    &\vdots \\
    \text{Fold 5} &= \{d_{4 \times \frac{n}{5}+1}, d_{4 \times \frac{n}{5}+2}, \dots, d_n\}
\end{align}
In each iteration, one of these folds is reserved for testing and the others for training, cycling through until each fold has been the test set once. The model's performance is then averaged over the \(k\) test sets to produce a single estimation.
The advantage of CV is its ability to utilize the entire dataset for both training and testing, which can be very helpfull when data is limited. By averaging over multiple trials, CV reduces the variance associated with a single random train-test split, providing a more robust measure of model performance.
However, CV can be computationally intensive, especially for large \(k\) values or large datasets. Also, unlike the bootstrap method where test data is entirely separate from training, there's an overlap in the training data across folds in CV. This means that CV may sometimes be overly optimistic about a model's performance.
In the context of our study, CV aids in parameter tuning, especially for models like Ridge and LASSO where the regularization strength is crucial.




\subsection{Bias-variance trade-off}
The Bias-varinace trade-off is a measuerment on how accuratly a model fit the real data. The bias-variance trade-off remains central in understanding model performance, striking a balance between two sources of errors that impact generalization. High bias suggests an oversimplification of the model, misrepresenting underlying patterns, while high variance indicates an overfitting, capturing noise as if it were a genuine pattern. Essentially, a model with low bias confidently navigates through the data but risks being misled by its own confidence, introducing possible overfitting. Conversely, a high-bias model may underperform due to its underfitting, missing critical data tendencies.
