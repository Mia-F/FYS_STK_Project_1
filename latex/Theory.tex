\thispagestyle{plain}
\externaldocument{appendix}
\subsection{Linear regression methods}

\noindent Linear regression is a foundational statistical modeling technique employed for the prediction of continuous target variables
based on one or more input features. It operates under the assumption that there exists a linear relationship between the input features
and the target variable, which is mathematically expressed as:

\begin{equation}\label{eq:linear_regression} 
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p 
\end{equation}

In this equation, $\hat{y}$ denotes the predicted target variable, while $x_1, x_2, \ldots, x_p$ represent the input features. The 
coefficients $\beta_0, \beta_1, \beta_2, \ldots, \beta_p$ are parameters that correspond to each respective feature. The primary objective 
of linear regression is to determine these coefficients to establish the optimal linear model that best fits the given data.

Linear regression encompasses several variants, each offering unique characteristics and advantages. In this particular context, 
we will narrow our focus to three fundamental techniques, Ordinary Least Squares (OLS), Ridge regression, and Lasso regression. These 
techniques will be employed in the context of analyzing the two-dimensional Franke function. 


\subsubsection{Ordinary least squares (OLS)}
\noindent Ordinary Least Squares (OLS) stands as a fundamental technique in regression
analysis, where the aim is to create a model that minimize the diffrense from
the observed data and the predicted model. 

%OLS is a linear model which means that it assumes that the regression function 
%is linear in the inputs $X_1, X_2 \dots X_n$

A linear regression model has the following form :
\begin{align}\label{eq:linear_model}
    f(\textbf{X}) = \beta_0 + \sum^{p}_{i=1} X_i \beta_i  \text{\cite{HASTIE2009}} \\%\cite{hastie_09_elements-of.statistical-learning} 
\end{align}

Where $\beta$ is the coefficients and X the design matrix. It is importent to note 
that thise method assume that either the function is linear or approxematly linear.


\subsubsection{Ordinary least squares (OLS)}
\noindent Ordinary Least Squares (OLS) stands as a fundamental technique in 
regression analysis, with the primary objective of creating a model that 
minimizes the discrepancy between observed data and the predicted model. 
This is achieved by estimating the coefficients in a linear regression model, 
as defined in Equation \ref{eq:linear_model}
, while minimizing the Mean Squared 
Error (MSE) %\cite{AY}.
To minimize the MSE, we calculate the optimal $\beta$ values. This involves 
taking the derivative of the cost function, as shown in Equation \ref{eq:cost_function}, 
and setting it to zero.

\begin{equation}\label{eq:cost_function}
C(\beta) = \frac{1}{n} \left\lbrace ( \textbf{y} - \textbf{X}\beta )^T (\textbf{y} - \textbf{X}\beta)\right\rbrace 
\end{equation}

\noindent By solving $\frac{\partial C(\beta)}{\partial \beta} = 0$, we determine that 
for OLS, the optimal $\beta$ values can be obtained by solving Equation
\ref{eq:optimal_beta_OLS}.

\begin{equation}\label{eq:optimal_beta_OLS}
\beta = (\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T \textbf{y} 
\end{equation}

\noindent Additionally, OLS possesses other properties that are relevant to this project,
such as the expectation value and variance. These properties come into play 
when applying the bias-variance trade-off to evaluate our models, and they can 
be expressed as follows:
\begin{align}
&\mathbb{E}_{OLS}(y_i) = \textbf{X}{i,*} \beta\\
&\mathbb{E}_{OLS}(\beta) = \beta\\
&\mathbb{V}_{OLS}(y_i) = \sigma^2\\
&\mathbb{V}_{OLS}(\hat{\beta}) = \sigma^2 (\textbf{X}^T \textbf{X})^{-1} \\
\end{align}
\noindent For a comprehensive step-by-step derivation of these expressions, please refer
to Appendix A. OLS, as a regression method, relies on specific assumptions 
about the data it is applied to. If these assumptions are not met by the 
dataset, the model's accuracy may be compromised. 
%The assumsions are %\cite{SBJ}:
%\begin{itemize}
%    \item The regression model is linear in the coefficients and the error term\\
%    \item The error term has a population mean of zero\\
%    \item All independent variables are uncorrelated with the error term\\
%    \item Observations of the error term are uncorrelated with each other\\
%    \item The error term has a constant variance\\
%    \item No independent variable is a perfect linear function of other explanatory variables\\
%\end{itemize}
\newline \newline
\textbf{Write about positiv and negativ things about OLS}
\newline \newline

\noindent While OLS is a straightforward and efficient linear regression method 
to implement, its limitations become particularly evident when dealing 
with datasets that contain substantial amounts of noise. In response to the 
limitations of OLS in noisy datasets, two other linear regression methods have been
developed namly Ridge and LASSO regression. 


\subsubsection{Ridge}
\noindent Ridge regression closely resembles OLS,but it have the addition of a term in the 
cost function, as depicted in the equation \ref{eq:cost_function Ridge}.
\begin{equation}\label{eq:cost_function Ridge}
    C(X,\beta) =  \left\lbrace ( \textbf{y} - \textbf{X}\beta )^T (\textbf{y} - \textbf{X}\beta)\right\rbrace + \lambda \beta^T \beta
\end{equation}
If we take the derivative of the cost function $\frac{\partial C(X, \beta)}{\partial \beta}$ and 
solve the equation when the derivative is zero, we then optain the equation for the optimal $\beta$:
\begin{equation}\label{eq:beta_ridge}
    \beta = (\textbf{X}^T \textbf{X} + \lambda \textbf{I})^{-1}\textbf{X}^T \textbf{y} 
\end{equation}
This extra term $\lambda \textbf{I}$ helps controling the size of the coefficents, if $\lambda$ is 
small, then ..

For the Bias variance trade off we will also need the following properties:
\begin{align}
    & \mathbb{E}(\beta_{Ridge}) = (X^T X + \lambda I )^{-1}(X^T X)\beta\\
    & \mathbb{V}(\beta_{Ridge}) = \sigma^2[X^T X + \lambda I]^{-1} X^T X \{[X^T X + \lambda I]^{-1}\}^T
\end{align}

Calculations for both the optimal $\beta$ and variance can be found in the appendix \ref{eq:bias_variance}

\subsubsection{LASSO}

\noindent Lasso, short for Least Absolute Shrinkage and Selection Operator, is a shrinkage method similar to Ridge Regression. Both
methods work towards minimizing the residual sum of squares while simultaneously incorporating a penalty term. The primary distinction
between these regression techniques lies in the way the penalty term is defined into the cost function.
\noindent In Lasso regression, the norm-2 vector $\sum_{i} \boldsymbol{\beta_i^2}$ , which is employed in Ridge Regression, is replaced by
the norm-1 vector $\sum_{i} 
|\boldsymbol{\beta_i}|$, leading to a nonlinear solution of $y_i$ \cite{SpringerCh3}.

\noindent The cost function for Lasso is shown under in equation \ref{eq:cost_function_Lasso}. 

\begin{equation}\label{eq:cost_function_Lasso}
    C(X,\beta)_\text{Lasso} =  \left\lbrace ( \textbf{y} - \textbf{X}\beta )^T (\textbf{y} - \textbf{X}\beta)\right\rbrace + \lambda||\boldsymbol{\beta}||_1  
\end{equation}

\noindent where $||\boldsymbol{\beta}||_1 = \sum_{i} |\boldsymbol{\beta_i}|$ which is found by taking the derivative with respect to $\boldsymbol{\beta}$ \cite{SpringerCh3}, 

\begin{equation}\label{eq:optimal_lasso}
    \mathbf{X}^T\mathbf{X}\boldsymbol{\beta} + \lambda sgn(\boldsymbol{\beta}) = 2\mathbf{X}^T\boldsymbol{y} 
\end{equation}

\noindent Lasso does not yield a closed-form expression due to a non linear solution, necessitating a numerical approach for its
determination. 
This method is essential in regression analysis because it carefully balance precision and clarity by penalizing the absolute size of
regression coefficients.(Kilde 2AM)

\subsection{Model evaluation methods}
\subsubsection{Mean Square Error}

\noindent Mean Squared Error,abbreviated MSE, is a commonly used risk metric that corresponds to the expected value of the squared (quadratic) error or loss (Kilde 3AM). It is defined as 

\begin{equation}\label{eq:MSE} 
 MSE(\boldsymbol{y},\tilde{\boldsymbol{y}}) = \frac{1}{n} \sum_{i=0}^{n-1}(y_i \tilde{y}_i)^2
\end{equation}

\noindent MSE measures the average of the squared differences between predicted values and actual outcomes. Therefore, the smaller the
value, the better the fit between the predictions and reality. Since the MSE value comes from the square of the errors between the actual
values and the predicted values, it cannot be negative. A perfect fit between the predictions and reality would yield an MSE value equal
to zero \cite{Week34}. A negative side to MSE is that it is not scaled. Outliers will thus contribute the same amount as any other point,
which can lead to an skewed MSE value. 

\subsubsection{$R^2$-Score}

\noindent The $R^2$-score function is a popular error metric that computes the coefficient of determination, which indicates how
accurately a model can predict future samples. The score typically ranges from 0 to 1, but it can also be negative. A score of 1 indicates
a perfect fit, while a negative value indicates an inadequate model. A constant model that always predicts the expected value of the target
variable, regardless of the input features, would receive an $R^2$-score of 0 \cite{Week34}.

\noindent The $R^2$-score is defined as 

\begin{equation}\label{eqR2} 
R^2(\boldsymbol{y}, \tilde{\boldsymbol{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2}
\end{equation}

\noindent where where we have defined the mean value of $\boldsymbol{y}$ as

\begin{equation} 
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
\end{equation}



\subsection{Resampling techniques}

\noindent The main restriction in machine learning is 
the amount of data points available to create the model out of. It may be the case where one have
done a costfull and time consuming experiments and are left with a small number of data. 
It is therfull extremely useful to have methods where one can reuse the data multiple times
thereby creating a relatively large dataset from the small number of datapoints. In this report
we are going to use two different methods, the first is called bootstrap and the second one is cross validation.

\subsubsection{Bootstrap} 
\noindent The bootstrap method is a resampling procedure that uses data from one 
sample to generate a sampling distribution by repeatedly taking random 
samples from the known sample, with replacement %\cite{PSU} 
This means that if we have a data set D with n data points.
The elements in this data set can be representatet in the following way:
\begin{align}
    D = {d_1, d_2, d_3, d_4, \dots, d_n}
\end{align}
Then by apllying the bootstarp method on this data set one possible output $D^{*}$ can be:
\begin{align}
    D^{*} = {d_3, d_n, d_4, d_4 , \dots, d_2}
\end{align}
From this example we see that one observation can appear multiple times in the new dataset.
We can take this method a step further and create "new" data points by extracting multible data points from 
the dataset and take the mean of all thise values:
\begin{align}
    d_{new} = \frac{1}{k}(d_1, \dots d_k)
\end{align}
This gives us a method of producing lots of "new" data-set from 
limited data points to train our model with. For each of this data-sets
the mean and standard deviation can be calculated to evaluate the model statistically. \cite{MLM}


One huge advatange of using the boostrap method is that the data can be split in to 
test and train before shuffeling the data, this means that the test data can be kept 
entirely separate from the creation of the model. When we den test the model it will
be on a dataset that has nothing to do with crating the model and will therfore show
how god the model represent real data. We draw samples from the same training set, to 
generate new dataset, and calculate the mean square error of each prediction. The initial 
distibution of our data is not known, however, when the sample size is large the mean 
square error will follow a normal distibution. The Central Limit theorem \cite{JAY2021CLT} states that for a 
large number of samples the law of large numbers \cite{JAY2021LLN} ensures the value of the mean converges toward the sample mean. 
When we are calculating the mean square error of our models prediction it will follow a 
normal distribution, and will be independent of distribution of the training data.


\subsubsection{Cross validation}
\noindent Cross validation (CV), specifically k-fold cross validation, is a technique designed to assess how well a model will generalize to an independent dataset. Unlike the bootstrap method, which samples with replacement, CV partitions the original dataset into \(k\) distinct subsets or "folds". One of these folds is kept as the test set, while the remaining \(k-1\) folds are used for training the model. This process is repeated \(k\) times, each time with a different fold reserved as the test set and the remaining folds as the training set. Given a dataset \(D\) with \(n\) data points represented as:
\begin{align}
    D = \{d_1, d_2, d_3, \dots, d_n\}
\end{align}
For \(k=5\), an example partition might be:
\begin{align}
    \text{Fold 1} &= \{d_1, d_2, \dots, d_{\frac{n}{5}}\} \\
    \text{Fold 2} &= \{d_{\frac{n}{5}+1}, d_{\frac{n}{5}+2}, \dots, d_{2 \times \frac{n}{5}}\} \\
    &\vdots \\
    \text{Fold 5} &= \{d_{4 \times \frac{n}{5}+1}, d_{4 \times \frac{n}{5}+2}, \dots, d_n\}
\end{align}
In each iteration, one of these folds is reserved for testing and the others for training, cycling through until each fold has been the test set once. The model's performance is then averaged over the \(k\) test sets to produce a single estimation.
The advantage of CV is its ability to utilize the entire dataset for both training and testing, which can be very helpfull when data is limited. By averaging over multiple trials, CV reduces the variance associated with a single random train-test split, providing a more robust measure of model performance.
However, CV can be computationally intensive, especially for large \(k\) values or large datasets. Also, unlike the bootstrap method where test data is entirely separate from training, there's an overlap in the training data across folds in CV. This means that CV may sometimes be overly optimistic about a model's performance.
In the context of our study, CV aids in parameter tuning, especially for models like Ridge and LASSO where the regularization strength is crucial.




\subsection{Bias-variance trade-off}
The Bias-varinace trade-off is a measuerment on how accuratly a model fit the real data. We can use the expression for expected value of 
the mean square error as a trade-off, using equation \ref{eq:bias_variance} for a continous function we get 
\begin{align}\label{eq:trade_off}
    \mathbb{E}((\boldsymbol{y} - \boldsymbol{\tilde{y}})^{2}) &= \mathbb{E}((f(\boldsymbol{x}) - \mathbb{E}(\boldsymbol{\tilde{y}}))^{2}) + \mathbb{V}(\boldsymbol{\tilde{y}}) + \sigma^{2}
\end{align}

