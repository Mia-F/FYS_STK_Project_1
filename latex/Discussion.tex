\thispagestyle{plain}
\noindent It was stated in the project description that we shoud use a nois that was given by
the normal distribution $\mathcal{N}(0,1)$. What we found was that since the Franke
Function when plotten for $x$ and $y$ in the range between $0$ and $1$ the maximal 
value of the Franke function becomes around $1.4$. When plottet with noise that had values
between $0$ and $1$ the plot of the franke function becomes unrecognizable as shown in figure \eqref{franke noise 1}. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{Figure_13.png}
	\caption{Plot showing the Frankece function with noise given by the normal distrubution $\mathcal{N}(0,1)$ }
	\label{franke noise 1}
\end{figure}
\noindent But if instead the normal distribution $\mathcal{N}(0,0.1)$ is 
used, the dataset becomes much easier to work with, and the results becomes 
much prettier (as shown in figure \eqref{franke noise 0.1}) since noise has a tendency of ruining everything.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{Figure_14.png}
	\caption{Plot showing the Frankece function with noise given by the normal distrubution $\mathcal{N}(0,0.1)$ }
	\label{franke noise 0.1}
\end{figure}
\noindent Now that we have gotten that out of the way, we can start actually
discussing the result of the regression analysis of the Franke function.

\subsection*{The Franke function}
\noindent We start with the analyses of the OLS implementation on the Franke
function. It is important to note that since the parameter x and y only had 
values from 0 to 1 when creating the Franke function, we did not see it necessary
to scale the data for any of the analysis of the franke function, since it in a way already is scaled. 
The first results are the OLS model of the franke function without noise. Figure \eqref{MSE and R2 OLS}
shows the MSE and R2 scores as a function of the polynomial degree used to create the design matrix.
The size of the dataset analysis was a $20 \cdot 20$ matrix, were $20\%$ was put aside as 
test data, and the rest was used for training. What we can see from the figure is that
the MSE is larger for lower polynomial degrees and starts to lower with higher values. 
At a fifth order polynomial degree the MSE approaches zero both for the test and training set.
This is as expected, since there is no noise present there will not be any posibility 
for overfitting the model to the noise. Therefore the MSE will become lower and 
lower for higher order polynomials as shown in figure \eqref{MSE and R2 OLS}. When 
it comes to the R2 score we see that it becomes closer and closer to one with higher 
order polynomials. This is due to our model becoming increasingly better with 
higher orders as shown form the MSE values.

Next we look at what happens when we introduce noise given by the normal distribution 
$\mathcal{N}(0,0.1)$. Figure \eqref{MSE and R2 OLS noise} show the MSE and the R2 score
for this case.Here we can see a classical example of overfitting the model to the noise.
We see that the MSE becomes better and better for the training set but for det test set
we see a sudden increas after fitting with a fourth order polynomial. This is due to
the amount of datapoints in the diffrent datasets, since the test dataset only have 
$20\%$ of the original data while the training set consist of the ramaining $80\%$.
If we increas the number of data points the predictet model for the test data will converge
towards that of the training model. 


\noindent From figure \eqref{MSE and R2 OLS} we see that when our data set does not include noise
the MSE for the training data gets lower with increas in complexity, while 
the R2 score gets closer and closer to 1. But when we introduce noise whe see from figure \eqref{MSE and R2 OLS noise}
that for our test data the MSE acctually increase with higher complexity. 

The dataset containing the test data consists of $20\%$ of the original
dataset. This implies that the number of points in the test data is 
significantly smaller than that in the training data. We anticipate 
that by increasing the number of data points, the MSE for the model 
created using the test data will converge towards that of the training
model.

What we can see from figure \eqref{beta OLS} is that for the higher order 
polynomials the values for the coefficients varies a lot in value. This is due to 
the regression method trying to fit the function to the noise, so this is a direct 
effect of overfitting the function with a to high order polynomial. From figure \eqref{beta OLS}
we see that the beta values start to vary at as low as a fourth order polynomial and that 
for a fifth order polynomial the variance start to become substantially large. This is 
also supported by the MSE plot shown in figure \eqref{MSE and R2 OLS noise}, where we can
see that the MSE starts to gradually increase after the third order polynomial. So we clearly
start to move in to the overfit area. 

We expect that if we set $\lambda = 0$ in our Ridge regression then the result 
will be the same as for OLS. From figur .. and ... we se that when lambda is 
set to zero then the model becoms equal to that for OLS, explenation...


\subsection*{Real terrain data}